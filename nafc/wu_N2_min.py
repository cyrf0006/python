
'''
Script to develop a stratification index at Station 27.
Would be useful for capelin forecast model and to revisit Wu paper.

**N2, strat and MLd time series generated by azmp_stn27_density.py

Frederic.Cyr@dfo-mpo.gc.ca

'''
import os
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
## import xarray as xr
## import datetime
## import os
## import matplotlib.dates as mdates
## from matplotlib.ticker import NullFormatter
## from matplotlib.dates import MonthLocator, DateFormatter
## import cmocean
import gsw
import pylab as plb
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit
from scipy import asarray as ar,exp
from lmfit.models import LorentzianModel
from scipy.interpolate import interp1d
from scipy.optimize import curve_fit
from scipy import optimize
from datetime import timedelta


### *** NEED TO FORMALIZE N2 vs strat 

g = 9.81
rho_0 = 1025
doy_offset = 18
zmax = .0003
zmon = 0.00005
REGION = '3LNO'
#depth_range = '5-50m'
#depth_range = '5-20m'
depth_range = '5-100m'
#depth_range = '5-100m'
#depth_range = '0-50m'
depth_range = '5-150m'

use_N2 = False #(otherwise use strat from density difference)
weekly_average= True
add_MLD = True
add_TS = False

# Gaussian function
def gaus(x,a,x0,sigma,off):
    return a*exp(-(x-x0)**2/(2*sigma**2))+off

# Function to calculate the exponential with constants a and b
def exponential(x, a, b):
    return a*np.exp(b*x)

# Logistic curve fitting
def logifunc(x,A,x0,k,off):
    return A / (1 + np.exp(-k*(x-x0)))+off

def smooth(x,window_len=11,window='hanning'):
    if x.ndim != 1:
        raise ValueError("smooth only accepts 1 dimension arrays.")
    if x.size < window_len:
        raise ValueError("Input vector needs to be bigger than window size.")
    if window_len<3:
            return x
    if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:
            raise ValueError("Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'")
    s=np.r_[2*x[0]-x[window_len-1::-1],x,2*x[-1]-x[-1:-window_len:-1]]
    if window == 'flat': #moving average
            w=np.ones(window_len,'d')
    else:  
            w=eval('np.'+window+'(window_len)')
    y=np.convolve(w/w.sum(),s,mode='same')
    return y[window_len:-window_len+1]

    
def piecewise_linear(x, x0, y0, k1, k2):
    return np.piecewise(x, [x < x0], [lambda x:k1*x + y0-k1*x0, lambda x:k2*x + y0-k2*x0])


if use_N2:
    df_N2 = pd.read_pickle('/home/cyrf0006/AZMP/state_reports/stn27/S27_N2_raw.pkl')
    if depth_range == '8-50m':
        df_strat = df_N2.iloc[:,7:50].mean(axis=1)
    elif depth_range == '5-50m':
        df_strat = df_N2.iloc[:,4:50].mean(axis=1)
    elif depth_range == '5-20m':
        df_strat = df_N2.iloc[:,4:20].mean(axis=1)
    elif depth_range == '5-100m':
        df_strat = df_N2.iloc[:,4:100].mean(axis=1)
    elif depth_range == '5-150m':
        df_strat = df_N2.iloc[:,4:150].mean(axis=1)
    elif depth_range == '8-100m':
        df_strat = df_N2.iloc[:,7:100].mean(axis=1)
    elif depth_range == '25-75m':
        df_strat = df_N2.iloc[:,24:5].mean(axis=1)
    elif depth_range == '5-175m':
        df_strat = df_N2.iloc.mean(axis=1)        
        
    # uncomment to save
    # df_N2.iloc[:,4:100].mean(axis=1).to_csv('S27_N2_timeseries_5-100m.csv', float_format='%.4f', index=True)
    # df_N2.iloc[:,4:50].mean(axis=1).to_csv('S27_N2_timeseries_5-50m.csv', float_format='%.4f', index=True)

else:
    ## load Stn 27 data
    if depth_range == '8-50m':
        df_strat = pd.read_pickle('/home/cyrf0006/AZMP/state_reports/stn27/S27_stratif_raw.pkl')
    elif depth_range == '5-50m':
        df_strat = pd.read_pickle('/home/cyrf0006/AZMP/state_reports/stn27/S27_stratif_5-50_raw.pkl')
    elif depth_range == '5-20m':
        df_strat = pd.read_pickle('/home/cyrf0006/AZMP/state_reports/stn27/S27_stratif_5-20_raw.pkl')
    elif depth_range == '5-100m':
        df_strat = pd.read_pickle('/home/cyrf0006/AZMP/state_reports/stn27/S27_stratif_5-100_raw.pkl')
    elif depth_range == '5-150m':
        df_strat = pd.read_pickle('/home/cyrf0006/AZMP/state_reports/stn27/S27_stratif_5-150_raw.pkl')
    elif depth_range == '5-175m':
        df_strat = pd.read_pickle('/home/cyrf0006/AZMP/state_reports/stn27/S27_stratif_5-175_raw.pkl')
    elif depth_range == '25-75m':
        df_strat = pd.read_pickle('/home/cyrf0006/AZMP/state_reports/stn27/S27_stratif_25-75_raw.pkl')
    else:
        print('Missing strat file')

    df_strat = df_strat*g/rho_0

# Weekly average
if weekly_average:
    df_strat = df_strat.resample('W').mean()

# load Stn 27 temp
if add_TS:
    CT = pd.read_pickle('/home/cyrf0006/AZMP/state_reports/stn27/S27_CT_raw.pkl')
    SA = pd.read_pickle('/home/cyrf0006/AZMP/state_reports/stn27/S27_SA_raw.pkl')
    # Weekly means
    if weekly_average:
        CT = CT.resample('W').mean()
        SA = SA.resample('W').mean()
    # Calculate T-S stratificaiton
    alpha = gsw.alpha(SA,CT,SA.columns)
    beta = gsw.beta(SA,CT,SA.columns)
    if depth_range == '5-10m':
        alpha = alpha[:,4:10].mean(axis=1)
        beta = beta[:,4:10].mean(axis=1)
        strat_T = -alpha*g*(CT.iloc[:,10] - CT.iloc[:,4])/5
        strat_S = beta*g*(SA.iloc[:,10] - SA.iloc[:,4])/5
    elif depth_range == '5-50m':
        alpha = alpha[:,4:50].mean(axis=1)
        beta = beta[:,4:50].mean(axis=1)
        strat_T = -alpha*g*(CT.iloc[:,50] - CT.iloc[:,4])/45
        strat_S = beta*g*(SA.iloc[:,50] - SA.iloc[:,4])/45    
    elif depth_range == '40-60m':
        alpha = alpha[:,39:60].mean(axis=1)
        beta = beta[:,39:60].mean(axis=1)
        strat_T = -alpha*g*(CT.iloc[:,60] - CT.iloc[:,39])/20
        strat_S = beta*g*(SA.iloc[:,60] - SA.iloc[:,39])/20    
    elif depth_range == '5-150m':
        alpha = alpha[:,4:150].mean(axis=1)
        beta = beta[:,4:150].mean(axis=1)
        strat_T = -alpha*g*(CT.iloc[:,150] - CT.iloc[:,4])/145
        strat_S = beta*g*(SA.iloc[:,150] - SA.iloc[:,4])/145    

## Load bloom params
#bloom = pd.read_csv('SpringBloom_parameters.csv')
bloom = pd.read_csv('BloomFitParams_Wu revisited_15Jun2021.csv')
bloom.set_index('Year', inplace=True)
# To only plot one region at a time (comment for all regions)
#bloom = bloom[bloom.Region==REGION]
# Modis vs SeaWiFS
#bloom_modis = bloom[bloom.Sensor=='Modis 4km']

## Load MDL data
df_MLD = pd.read_pickle('/home/cyrf0006/AZMP/state_reports/stn27/S27_MLD_raw.pkl')
if weekly_average:
    df_MLD = df_MLD.resample('W').mean()

## Load TS data
df_ = pd.read_pickle('/home/cyrf0006/AZMP/state_reports/stn27/S27_MLD_raw.pkl')
if weekly_average:
    df_MLD = df_MLD.resample('W').mean()
    
## ## ---- Stratification Climatology Figure ---- ##
## df_doy = df_strat.copy()
## df_doy = df_doy[(df_doy.index.year>=1991) & (df_doy.index.year<=2020)]
## df_doy.index = df_doy.index.dayofyear
## df_doy = df_doy.groupby('time').mean()
## df_doy = df_doy.interpolate()
## # plot
## plt.close('all')
## fig, ax = plt.subplots(nrows=1, ncols=1)
## df_doy.plot(linestyle=' ', marker='.')    
## df_doy.rolling(30, center=True).mean().plot(linestyle='-', linewidth=3, color='orange')
## plt.ylabel(r'$\rm d \sigma_0 / dz$', color='orange')
## plt.xlabel('DOY')
## if add_TS:
##     Ts_doy = strat_T.copy()
##     Ss_doy = strat_S.copy()
##     Ts_doy = Ts_doy[(Ts_doy.index.year>=1991) & (Ts_doy.index.year<=2020)]
##     Ss_doy = Ss_doy[(Ss_doy.index.year>=1991) & (Ss_doy.index.year<=2020)]
##     # Interpolate T
##     Ts_doy.index = Ts_doy.index.dayofyear
##     Ts_doy = Ts_doy.groupby('time').mean()
##     #Ts_doy = np.sqrt(Ts_doy) # squareroot
##     Ts_doy = Ts_doy.interpolate()
##     # Interpolate S
##     Ss_doy.index = Ss_doy.index.dayofyear
##     Ss_doy = Ss_doy.groupby('time').mean()
##     #Ss_doy = np.sqrt(Ss_doy) # squareroot
##     Ss_doy = Ss_doy.interpolate()
##     # plot
##     if use_N2:
##         ax2=ax
##         ax2.set_ylabel(r'$log_{10}(N, N_T, N_S~/~s^{-1})$', color='black')
##         ax2.set_yscale('log')
##         YLIM = [1e-8, 5e-4]
##     else:
##         ax2 = ax.twinx()
##         YLIM = [.005, .032]      
##     Ts_doy.rolling(30, center=True).mean().plot(linestyle='-', linewidth=3, color='red')
##     Ss_doy.rolling(30, center=True).mean().plot(linestyle='-', linewidth=3, color='blue')

## if add_MLD:
##     mld_doy = df_MLD.copy()
##     mld_doy = mld_doy[(mld_doy.index.year>=1991) & (mld_doy.index.year<=2020)]
##     mld_doy.index = mld_doy.index.dayofyear
##     mld_doy = mld_doy.groupby('time').mean()
##     mld_doy = mld_doy.interpolate()
##     # plot
##     ax3 = ax.twinx()
##     # move the the second axes outwards
##     #ax3.spines["right"].set_position(("axes", 1.2))
##     mld_doy.plot(ax=ax3, linestyle=' ', marker='.', color='cyan')    
##     mld_doy.rolling(30, center=True).mean().plot(ax=ax3, linestyle='-', linewidth=3, color='darkgray')
##     ax3.set_ylabel('MLD (m)', color='darkgray')
##     ax3.invert_yaxis()

## # Load bloom timing
## bloomt = pd.read_csv('MeanTimingMax.csv')
## bloomt.set_index('Region', inplace=True)
## doy = bloomt.loc[REGION]
## init = pd.to_datetime(1900 * 1000 + doy.meanIniation, format='%Y%j')
## init_low = init - timedelta(days=doy['sdIni'])
## init_high = init + timedelta(doy['sdIni'])
## ax.fill_between([init_low.dayofyear, init_high.dayofyear], [YLIM[0], YLIM[0]], [YLIM[1], YLIM[1]], facecolor='green', interpolate=True , alpha=.3)
## max = pd.to_datetime(1900 * 1000 + doy.meanMax, format='%Y%j')
## max_low = max - timedelta(days=doy['sdMax'])
## max_high = max + timedelta(doy['sdMax'])
## ax.fill_between([max_low.dayofyear, max_high.dayofyear], [YLIM[0], YLIM[0]], [YLIM[1], YLIM[1]], facecolor='red', interpolate=True , alpha=.3)
## ax.set_ylim(YLIM)
## plt.grid()
## # Save Figure
## fig.set_size_inches(w=8, h=6)
## outfile_year = 'Climatological_strat.png'
## fig.savefig(outfile_year, dpi=200)
## os.system('convert -trim ' + outfile_year + ' ' + outfile_year)


## --- Annual plot + fit --- ##
years = np.arange(1950, 2017)
bloom_predict = years*np.nan
bloom_predict2 = years*np.nan
for iyear, year in enumerate(years):
    print(str(year))
    # Get annual values
    df0 = df_strat[df_strat.index.year==year]
    df0.dropna(inplace=True)
    df0.sort_index(inplace=True)
    df0.index = df0.index.dayofyear
    df0 = df0.groupby('time',as_index=True).mean()
    # add previous year (df_m1 = df_-1)
    df_m1 = df_strat[df_strat.index.year==year-1]
    df_m1.dropna(inplace=True)
    df_m1.sort_index(inplace=True)
    df_m1.index = df_m1.index.dayofyear
    df_m1 = df_m1.groupby('time',as_index=True).mean()
    df_m1.index = df_m1.index - 365 # negative index for previous year
    # Keep from one max to the other
    df0 = df0.loc[0:df0.argmax()]
    df_m1 = df_m1.loc[df_m1.argmax():]
    df = pd.concat([df_m1, df0], axis=0)   

    # params for fit
    x = df.index.values
    y = df.values
    xi = np.arange(x.min(), x.max())
    xi_mid = np.arange(x.min()+.5, x.max()-.5)
    
    # Fit polynomial
    p = np.polyfit(x, y, 2)

    #result.plot_fit()
    plt.close('all')
    fig, ax = plt.subplots(nrows=1, ncols=1)
    plt.plot(x,y,'b+:',label='data')
    #df.rolling(10, center='true').mean().plot()
    plt.plot(xi, np.polyval(p, xi),'r-',label='polyfit')
    B = bloom.loc[year]
    # To only plot one region at a time (comment for all regions)
    B = B[B.Region==REGION]
    try:
        TMP_b = B[B.Sensor=='MODIS 4km']
        Rb = TMP_b.Region
        m = B[B.Sensor=='MODIS 4km']['t[start]'].values
        for i, r in enumerate(Rb):
            plt.plot([m[i],m[i]],[0,zmax], '--k', zorder=10)
            plt.text(m[i], zmax, 'MODIS', verticalAlignment='bottom', horizontalAlignment='left')
    except:
        print(' -> No MODIS')

    try:
        TMP_v = B[B.Sensor=='SeaWiFS 4km']
        Rv = TMP_v.Region
        v = B[B.Sensor=='SeaWiFS 4km']['t[start]'].values
        for i, r in enumerate(Rv):
            plt.plot([v[i],v[i]],[0,zmax], '--b', zorder=10)
            plt.text(v[i], zmax+.00001, 'SeaWiFS', verticalAlignment='bottom', horizontalAlignment='left', color='b')
    except:
        print(' -> No SeaWiFS')        

    # Add detection threshold
    # Predict as minimum
    yi = np.polyval(p, xi)
    predict = xi[yi.argmin()]
    plt.plot([predict, predict],[0,zmax], 'r')
    bloom_predict[iyear] = predict


    ## fig, (ax0, ax1) = plt.subplots(1, 2)
    ## xs = x
    ## ys = y
    ## dys = np.gradient(ys, xs)
    ## rgr = DecisionTreeRegressor(max_leaf_nodes=n_seg)
    ## rgr.fit(xs.reshape(-1, 1), dys.reshape(-1, 1))
    ## dys_dt = rgr.predict(xs.reshape(-1, 1)).flatten()

    ## ys_sl = np.ones(len(xs)) * np.nan
    ## for y in np.unique(dys_dt):
    ##     msk = dys_dt == y
    ##     lin_reg = LinearRegression()
    ##     lin_reg.fit(xs[msk].reshape(-1, 1), ys[msk].reshape(-1, 1))
    ##     ys_sl[msk] = lin_reg.predict(xs[msk].reshape(-1, 1)).flatten()
    ##     ax0.plot([xs[msk][0], xs[msk][-1]],
    ##             [ys_sl[msk][0], ys_sl[msk][-1]],
    ##             color='r', zorder=1)

    ## ax0.set_title('values')
    ## ax0.scatter(xs, ys, label='data')
    ## ax0.scatter(xs, ys_sl, s=3**2, label='seg lin reg', color='g', zorder=5)
    ## ax0.legend()

    ## ax1.set_title('slope')
    ## ax1.scatter(xs, dys, label='data')
    ## ax1.scatter(xs, dys_dt, label='DecisionTree', s=2**2)
    ## ax1.legend()

    ## # Piecewise fit #3
    ## from numpy.linalg import lstsq

    ## ramp = lambda u: np.maximum( u, 0 )
    ## step = lambda u: ( u > 0 ).astype(float)

    ## def SegmentedLinearReg( X, Y, breakpoints ):
    ##     nIterationMax = 100

    ##     breakpoints = np.sort( np.array(breakpoints) )

    ##     dt = np.min( np.diff(X) )
    ##     ones = np.ones_like(X)

    ##     for i in range( nIterationMax ):
    ##         # Linear regression:  solve A*p = Y
    ##         Rk = [ramp( X - xk ) for xk in breakpoints ]
    ##         Sk = [step( X - xk ) for xk in breakpoints ]
    ##         A = np.array([ ones, X ] + Rk + Sk )
    ##         p =  lstsq(A.transpose(), Y, rcond=None)[0] 

    ##         # Parameters identification:
    ##         a, b = p[0:2]
    ##         ck = p[ 2:2+len(breakpoints) ]
    ##         dk = p[ 2+len(breakpoints): ]

    ##         # Estimation of the next break-points:
    ##         newBreakpoints = breakpoints - dk/ck 

    ##         # Stop condition
    ##         if np.max(np.abs(newBreakpoints - breakpoints)) < dt/5:
    ##             break

    ##         breakpoints = newBreakpoints
    ##     else:
    ##         print( 'maximum iteration reached' )

    ##     # Compute the final segmented fit:
    ##     Xsolution = np.insert( np.append( breakpoints, max(X) ), 0, min(X) )
    ##     ones =  np.ones_like(Xsolution) 
    ##     Rk = [ c*ramp( Xsolution - x0 ) for x0, c in zip(breakpoints, ck) ]

    ##     Ysolution = a*ones + b*Xsolution + np.sum( Rk, axis=0 )

    ##     return Xsolution, Ysolution

    ## from scipy import interpolate
    ## #fig, axes = plt.subplots(3)
    ## tck = interpolate.splrep(x, y, k=3, s=0.001)
    ## xnew = np.linspace(-150, 250)
    ## ynew = interpolate.splev(xnew, tck, der=0)
    ## X = xnew
    ## Y = ynew #smooth(y, window_len=11)
    
    ## initialBreakpoints = [-75, 0, 75, 150]
    ## plt.plot( *SegmentedLinearReg( X, Y, initialBreakpoints ), '-c' );
    ## plt.xlabel('X'); plt.ylabel('Y');


    ## spline and 2nd derivative
    ## keyboard
    #from scipy import interpolate
    #fig, axes = plt.subplots(3)
    #tck = interpolate.splrep(x, y, k=3, s=0.001)
    #xnew = np.linspace(-250, 250)
    #ynew = interpolate.splev(xnew, tck, der=0)
    ## axes[0].plot(x, y, 'x', label = 'data')
    ## axes[0].plot(xnew, interpolate.splev(xnew, tck, der=0), label = 'Fit')
    ## axes[0].grid()
    ## axes[1].plot(x, interpolate.splev(x, tck, der=1), label = '1st dev')
    ## axes[1].grid()
    ## dev_2 = interpolate.splev(x, tck, der=2)
    ## axes[2].plot(x, dev_2, label = '2st dev')
    ## turning_point_mask = dev_2 == np.amax(dev_2)
    ## axes[2].plot(x[turning_point_mask], dev_2[turning_point_mask],'rx', label = 'Turning point')
    ## axes[2].grid()


    #predict2 = x[turning_point_mask]  
    #plt.plot([predict2, predict2],[0,zmax], 'm')


    plt.legend()
    plt.title('Fit on stratification - ' + str(year))
    plt.xlabel('doy')
    plt.ylabel(r'$\rm log_{10}(\tilde{N}^2 ~/~ s^{-2})$')
    plt.grid()
    # Save Figure
    fig.set_size_inches(w=12, h=6)
    outfile_year = 'N2_bloom_' + str(year) + '_' + REGION + '_' + depth_range + '.png'
    fig.savefig(outfile_year, dpi=200)
    os.system('convert -trim ' + outfile_year + ' ' + outfile_year)


# Save prediction
df_predict = pd.DataFrame(bloom_predict, index=years, columns=['predict'])
df_predict.index.name='Year'

df_predict.to_csv('bloom_predict_gaussian_' + REGION + '_' + depth_range + '.csv')    
keyboard

## ---- Correlation with prediction ---- ##
## Add bloom for selected region
bloom = bloom[bloom.Region==REGION]
# Select sensors
df_predict['Modis'] = bloom[bloom.Sensor=='MODIS 4km']['t[start]']
df_predict['SeaWiFS'] = bloom[bloom.Sensor=='SeaWiFS 4km']['t[start]']
both = df_predict.loc[: , "Modis":"SeaWiFS"]
df_predict['Both'] = both.mean(axis=1)
# Compute anomaly
anom = (df_predict - df_predict.mean(axis=0)) / df_predict.std(axis=0)
anom = anom[['predict', 'Both']]                                                  
anom2 = anom.copy()
#anom2['tice'] = tice_anom

## Get tice
## tice = pd.read_csv('capelin-m1-2020.csv')
## tice.set_index('year', inplace=True)
## tice.index.name='Year'
## tice = tice.tice
## tice.name='tice'                                                           
## tice_clim = tice[(tice.index>=1991) & ((tice.index<=2020))]
## tice_anom = (tice - tice_clim.mean(axis=0)) / tice_clim.std(axis=0)

## Get fall condition
fc = pd.read_csv('condition_ag1_2_MF_out.csv')
fc.name = 'CapelinCond'
fc.set_index('year', inplace=True)
fc.index.name='Year'
fc_anom = (fc - fc.mean(axis=0)) / fc.std(axis=0)
anom2['CapelinCond'] = fc_anom

# Drop some years (late occupation)
df_predict.drop(1998, inplace=True)

# Correlation
corr = df_predict.corr()
corr_anom = anom.corr()

from scipy.stats import pearsonr
def calculate_pvalues(df):
    df = df.dropna()._get_numeric_data()
    dfcols = pd.DataFrame(columns=df.columns)
    pvalues = dfcols.transpose().join(dfcols, how='outer')
    for r in df.columns:
        for c in df.columns:
            pvalues[r][c] = round(pearsonr(df[r], df[c])[1], 4)
    return pvalues

# add climate index.
corrMatrix = df_predict.corr().round(2)
pvalues = calculate_pvalues(df_predict)


# plot
plt.close('all')
fig, ax = plt.subplots(nrows=1, ncols=1)
df_predict.plot(ax=ax)
ax.grid()
ax.text(2004, 105, 'corr with Modis = ' + str(np.round(corr.iloc[0,1],3)))
ax.text(2004, 100, 'corr with SeaWiFS = ' + str(np.round(corr.iloc[0,2],3)))
ax.text(2004, 95, 'corr with both = ' + str(np.round(corr.iloc[0,3],3)))
plt.ylabel('bloom init (DOY)')
# Save Figure
fig.set_size_inches(w=12, h=6)
outfile_year = 'Predict_bloom_gauss' + REGION + '_' + depth_range + '.png'
fig.savefig(outfile_year, dpi=200)
os.system('convert -trim ' + outfile_year + ' ' + outfile_year)

# plot
plt.figure()
fig, ax = plt.subplots(nrows=1, ncols=1)
anom.plot(ax=ax, linewidth=2)
plt.legend([r'DOY of $\rm \tilde{N}^2_{min}$', 'DOY of bloom initiation'])
ax.grid()
#ax.text(2012, 2.25, ' r  = ' + str(np.round(corrMatrix.iloc[0,3],3)) + ' [p = ' + str(np.round(pvalues.iloc[0,3],3)) + ']', FontSize=14)
ax.text(2012, 2.25, ' r  = ' + str(np.round(corrMatrix.iloc[0,3],3)), FontSize=14)
plt.ylabel('standardized anomaly', FontSize=14)
plt.xlabel(' ')
# Save Figure
fig.set_size_inches(w=7, h=4)
outfile_year = 'Bloom_timing_anom' + REGION + '_' + depth_range + '.png'
fig.savefig(outfile_year, dpi=200)
os.system('convert -trim ' + outfile_year + ' ' + outfile_year)

# plot add tice
plt.figure()
fig, ax = plt.subplots(nrows=1, ncols=1)
anom2.plot(ax=ax, linewidth=2)
#plt.legend(['predict. from stratification', 'Modis / SeaWiFS'])
ax.grid()
ax.text(1998, 2.1, ' r  = ' + str(np.round(corrMatrix.iloc[0,3],3)) + ' [p = ' + str(np.round(pvalues.iloc[0,3],3)) + ']', FontSize=14)
plt.ylabel('bloom initiation std anomaly', FontSize=14)
# Save Figure
fig.set_size_inches(w=12, h=6)
outfile_year = 'Bloom_timing_anom_tice' + REGION + '_' + depth_range + '.png'
fig.savefig(outfile_year, dpi=200)
os.system('convert -trim ' + outfile_year + ' ' + outfile_year)

# montage N2_bloom_1998_3LNO_5-150m.png  N2_bloom_2002_3LNO_5-150m.png N2_bloom_2006_3LNO_5-150m.png N2_bloom_2010_3LNO_5-150m.png -tile 2x2 -geometry +10+10  -background white  montage_strat_fits.png


# montage N2_bloom_1998_3LNO_5-150m.png N2_bloom_1999_3LNO_5-150m.png N2_bloom_2000_3LNO_5-150m.png N2_bloom_2001_3LNO_5-150m.png N2_bloom_2002_3LNO_5-150m.png N2_bloom_2003_3LNO_5-150m.png N2_bloom_2004_3LNO_5-150m.png N2_bloom_2005_3LNO_5-150m.png -tile 2x4 -geometry +10+10  -background white  montage_strat_fits1.png

#montage N2_bloom_2006_3LNO_5-150m.png N2_bloom_2007_3LNO_5-150m.png N2_bloom_2008_3LNO_5-150m.png N2_bloom_2009_3LNO_5-150m.png N2_bloom_2010_3LNO_5-150m.png N2_bloom_2011_3LNO_5-150m.png N2_bloom_2012_3LNO_5-150m.png N2_bloom_2013_3LNO_5-150m.png -tile 2x4 -geometry +10+10  -background white  montage_strat_fits2.png

#montage  N2_bloom_2014_3LNO_5-150m.png N2_bloom_2015_3LNO_5-150m.png N2_bloom_2016_3LNO_5-150m.png -tile 1x3 -geometry +10+10  -background white  montage_strat_fits3.png
